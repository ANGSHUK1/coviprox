# import the necessary packages
import torch
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# import os

lb = LabelBinarizer()

# initialize the initial learning rate, number of epochs to train for,
# and batch size
INIT_LR = 1e-4
EPOCHS = 20
BS = 32


data = np.load('/Users/rishabaryan/Downloads/CZ4042 GROUP PROJECT/Source code and datasets/data_mask_nomask/datafile.npy')
labels = np.load('/Users/rishabaryan/Downloads/CZ4042 GROUP PROJECT/Source code and datasets/data_mask_nomask/labelfile.npy')

(trainX, testX, trainY, testY) = train_test_split(data, labels,
	test_size=0.20, stratify=labels, random_state=42)

trainX = torch.tensor(trainX, dtype=torch.float)
testX = torch.tensor(testX, dtype=torch.float)
trainY = torch.tensor(trainY, dtype=torch.float)
testY = torch.tensor(testY, dtype=torch.float)
# construct the training image generator for data augmentation


def aug(datafile):
    return torch.utils.data.DataLoader(
        datasets.ImageFolder(valdir, transforms.Compose([
            transforms.RandomAffine(degrees = 0, shear = 0.15),
            transforms.RandomRotation(20),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0, 0, 0],
                                     std=[1, 1, 1]),
            transforms.RandomHorizontalFlip(p=0.5),
        ])),
        batch_size=BS, shuffle=False,
        #num_workers=args.workers,
        pin_memory=True)

trainXdl = aug(trainX)
testXdl = aug(testX)



# load the MobileNetV2 network, ensuring the head FC layer sets are
# left off

model = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)

# construct the head of the model that will be placed on top of the
# the base model

class ModelClass(nn.Module):
    def __init__(self):
        super(TheModelClass, self).__init__()
        self.AvPool1 = nn.AvgPool2d((7,7))
        self.flat = nn.Flatten()
        self.lin1 = nn.linear(150528,128)
        self.drop1 = nn.Dropout()
        self.softmax1 = nn.Softmax()
        
    def forward(self, x):
        x = F.relu(self.AvPool1(x))
        x = self.flat(x)
        x = F.relu(self.lin1(x))
        x = self.drop1(x)
        x = self.softmax1(x)

new_model = TheModelClass()

headModel = model(trainX)
new_output = new_model(headModel)

# place the head FC model on top of the base model (this will become
# the actual model we will train)
#model = Model(inputs=baseModel.input, outputs=headModel)

print(new_output)

# loop over all layers in the base model and freeze them so they will
# *not* be updated during the first training process
for param in model.parameters():
    param.requires_grad = False

# compile our model
print("[INFO] compiling model...")

opt = torch.optim.Adam(new_model.parameters(),lr = INIT_LR, weight_decay = INIT_LR / EPOCHS)

criterion = torch.nn.BCELoss()

def train(model, inputs, output, loss, optimizer):
    correct = 0
    model.train()
    optimizer.zero_grad()
    train_output = model(inputs)
    train_loss = loss(train_output, output)
    correct += (torch.argmax(train_output, dim=1) == output).float().sum()
    train_loss.backward()
    optimizer.step()
    return train_loss, correct

def evaluate(model, inputs, output, loss):
    correct = 0
    model.eval()
    eval_outut = model(inputs)
    eval_loss = loss(eval_output, output)
    correct += (torch.argmax(eval_output, dim=1) == output).float().sum()
    return eval_loss, correct
    
# train the head of the networkd
print("[INFO] training head...")
finale = pd.DataFrame(columns = ['epoch', 'train_loss','val_loss', 'train_acc', 'val_acc'])
for epoch in range(EPOCHS):
    # go through all the batches generated by dataloader
    for i, (inputs, targets) in enumerate(trainXdl):
            # clear the gradients
            train_loss, train_acc = train(new_model, inputs, targets, criterion, opt)
            
    # go through all the batches generated by dataloader
    for i, (inputs, targets) in enumerate(trainXdl):
            # clear the gradients
            val_loss, val_acc = evaluate(new_model, inputs, targets, criterion)
    print('epoch :{}, train_loss:{}, eval_loss:{}, train_acc:{}, eval_acc:{}'.format(epoch, train_loss,val_loss, train_acc, val_acc ))
    finale = finale.append({'epoch':epoch, 'train_loss':train_loss,'val_loss':val_loss, 'train_acc':train_acc, 'val_acc':val_acc}, ignore_index = True)



# make predictions on the testing set
print("[INFO] evaluating network...")
pred = torch.argmax(new_model(testX), dim = 1)




# for each image in the testing set we need to find the index of the
# label with corresponding largest predicted probability


# show a nicely formatted classification report
# print(classification_report(testY.argmax(axis=1), predIdxs,
# 	target_names=lb.classes_))

# serialize the model to disk
print("[INFO] saving mask detector model...")
torch.save_dict(model,"/output/mask_detector.model", save_format="h5")
finale.to_csv('output_results.csv')

# plot the training loss and accuracy
