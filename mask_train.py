# import the necessary packages
import torch
from torch import nn
from torch.nn import parameter
from torchvision import transforms, datasets
import torch.nn.functional as F  
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torchvision
from torchvision.transforms.transforms import ToPILImage, ToTensor
from data_processing import create_dataset, get_transform_object, create_data
from model import finetune_model
from metric import accuracy, f1_score
# import os

lb = LabelBinarizer()
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# initialize the initial learning rate, number of epochs to train for,
# and batch size
INIT_LR = 1e-4
EPOCHS = 20
train_BS = 32
test_BS = 8
val_BS = 8
PATH = '/data/dest_folder'

# data = np.load('/data/datafile.npy')
# labels = np.load('/data/labelfile.npy')

# (trainX, testX, trainY, testY) = train_test_split(data, labels,
# 	test_size=0.20, stratify=labels, random_state=42)#num_workers=args.workers,
# trainX = torch.tensor(trainX, dtype= torch.float)
# testX = torch.tensor(testX, dtype= torch.float)
# trainY = torch.tensor(trainY, dtype = torch.float)
# testY = torch.tensor(testY, dtype = torch.float)
# # construct the training image generator for data augmentation
# trainX = torch.transpose(trainX, 1, 3)
# testX = torch.transpose(testX, 1, 3)

trainX, trainY, valX, valY, testX,testY = create_data(PATH)



train_dataset_pytorch = create_dataset(trainX, trainY)
test_dataset_pytorch = create_dataset(testX, testY)
val_dataset_pytorch = create_dataset(valX, valY)


def aug(datafile, BS):
    return torch.utils.data.DataLoader(
            datafile,
            batch_size=BS, 
            shuffle=True,
            #num_workers=args.workers,
            pin_memory=False)

trainXdl = aug(train_dataset_pytorch, train_BS)
testXdl = aug(test_dataset_pytorch, test_BS)
valXdl = aug(val_dataset_pytorch, val_BS)



# load the MobileNetV2 network, ensuring the head FC layer sets are
# left off
model = torchvision.models.vgg19_bn(pretrained= True)
model.classifier = finetune_model()
model = model.to(device)
model_name = 'vgg'

# construct the head of the model that will be placed on top of the
# the base model


# place the head FC model on top of the base model (this will become
# the actual model we will train)
#model = Model(inputs=baseModel.input, outputs=headModel)

# print(new_output)

# loop over all layers in the base model and freeze them so they will
# *not* be updated during the first training process


# compile our model
print("[INFO] compiling model...")

opt = torch.optim.Adam(model.parameters(), lr = INIT_LR)

criterion = torch.nn.CrossEntropyLoss()

def train(model, inputs, output, loss, optimizer):
    model.train()
    optimizer.zero_grad()
    train_output = model(inputs)
    train_loss = loss(train_output, output)
    acc = accuracy(train_output, output)
    train_loss.backward()
    optimizer.step()
    return train_loss, acc

def evaluate(model, inputs, output, loss):
    model.eval()
    eval_output = model(inputs)
    eval_loss = loss(eval_output,output)
    acc = accuracy(eval_output, output)
    return eval_loss, acc
    
# train the head of the networkd

transform = get_transform_object(True)
transform_eval = get_transform_object(False)

print("[INFO] training head...")
finale = pd.DataFrame(columns = ['epoch', 'train_loss','val_loss', 'train_acc', 'val_acc'])

for epoch in range(EPOCHS):
    train_loss_list = []; train_acc_list = []; val_loss_list = []; val_acc_list = []
    # go through all the batches generated by dataloader
    for i,(input, targets) in enumerate(trainXdl):
            # clear the gradients   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            for i in range(input.shape[0]):
                input[i] = transform(input[i])
            train_loss, train_acc = train(model, input.to(device), targets.to(device), criterion, opt)
            train_loss_list.append(train_loss); train_acc_list.append(train_acc)
            print('epoch:{}, train_loss:{}, train_acc:{}'.format(epoch+1, train_loss, train_acc))
    
  
        
    for i,(input_eval, targets_eval) in enumerate(valXdl):
            # clear the gradients 
            for i in range(input_eval.shape[0]):
                input_eval[i] = transform_eval(input_eval[i])
            val_loss, val_acc = evaluate(model, input_eval.to(device), targets_eval.to(device), criterion)
            val_loss_list.append(train_loss); val_acc_list.append(val_acc)
            print('epoch:{}, val_loss:{}, val_acc:{}'.format(epoch+1, val_loss, val_acc))
    

    print('epoch:{}, train_loss:{}, eval_loss:{}, train_acc:{}, eval_acc:{}'.format(epoch, sum(train_loss_list)/len(train_loss_list), sum(val_loss_list)/len(val_loss_list), sum(train_acc_list)/len(train_acc_list), sum(val_acc_list)/len(val_acc_list)))
    finale = finale.append({'epoch':epoch+1, 'train_loss':sum(train_loss_list)/len(train_loss_list),'val_loss':sum(val_loss_list)/len(val_loss_list), 'train_acc':sum(train_acc_list)/len(train_acc_list), 'val_acc':sum(val_acc_list)/len(val_acc_list)}, ignore_index = True)  


# # make predictions on the testing set




print('***********************TESTING NOW*******************************************')
test_acc_list = []; f1_list = []
for i, (input_test, targets_test) in enumerate(testXdl):
    with torch.no_grad():
        for num in range(input_test.shape[0]):
            input_test[num] = transform_eval(input_test[num])
        test_output = model(input_test.to(device))
        print('actual', targets_test)
        print('pred', torch.argmax(test_output, dim=1))
        test_acc = accuracy(test_output, targets_test.to(device))
        test_f1 = f1_score(targets_test, torch.argmax(test_output, dim=1))
        test_acc_list.append(test_acc); f1_list.append(test_f1)
    print('batch no:{}, test_acc:{}'.format(i, test_acc))
print('test accuracy : {}, f1:{}'.format(sum(test_acc_list)/len(test_acc_list, sum(f1_list)/len(f1_list))))

 
# print("[INFO] evaluating network...")
# pred = torch.argmax(new_model(testX), dim = 1)




# # for each image in the testing set we need to find the index of the
# # label with corresponding largest predicted probability


# # show a nicely formatted classification report
# # print(classification_report(testY.argmax(axis=1), predIdxs,
# # 	target_names=lb.classes_))

# serialize the model to disk
print("[INFO] saving mask detector model...")
print(model_name)
torch.save(model.state_dict(),"/output/mask_detector_model.pt")
finale.to_csv('/output/output_results.csv')

# plot the training loss and accuracy
